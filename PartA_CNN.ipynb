{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T09:57:47.060705Z",
     "iopub.status.busy": "2023-04-09T09:57:47.060369Z",
     "iopub.status.idle": "2023-04-09T09:59:49.932773Z",
     "shell.execute_reply": "2023-04-09T09:59:49.931619Z",
     "shell.execute_reply.started": "2023-04-09T09:57:47.060672Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "%matplotlib inline\n",
    "\n",
    "prefix='/kaggle/input/inaturalist12k/Data/inaturalist_12K/'\n",
    "\n",
    "data_prefix='/kaggle/working/'\n",
    "classes=['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n",
    "\n",
    "    \n",
    "flag=os.path.exists(data_prefix+'splittedVal')\n",
    "\n",
    "\n",
    "## Splitting train to train(80%) and valid(20%)\n",
    "\n",
    "valid_split=0.2\n",
    "if not flag:\n",
    "    for each in ['train','val']:\n",
    "        shutil.copytree(prefix+each,data_prefix+each)\n",
    "    os.mkdir(data_prefix+\"splittedVal\")\n",
    "    for each in classes:\n",
    "        images = os.listdir(data_prefix+'train/'+each+'/')\n",
    "        random.shuffle(images)\n",
    "        valid_till=int(len(images)*valid_split)\n",
    "        os.mkdir(data_prefix+'splittedVal/'+each)\n",
    "        for i in range(valid_till):\n",
    "            shutil.move(data_prefix+'train/'+each+\"/\"+images[i],data_prefix+'splittedVal/'+each)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T09:59:49.935665Z",
     "iopub.status.busy": "2023-04-09T09:59:49.934893Z",
     "iopub.status.idle": "2023-04-09T09:59:52.350798Z",
     "shell.execute_reply": "2023-04-09T09:59:52.349351Z",
     "shell.execute_reply.started": "2023-04-09T09:59:49.935612Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_distribution(dataset):\n",
    "    '''\n",
    "    exploring data distibution for each of the classes\n",
    "    '''\n",
    "    print(\"Total number of samples :\",len(dataset.targets))\n",
    "    print(\"Number of classes :\",len(set(dataset.targets)),'\\n')\n",
    "    idx_to_class = {v:k for k, v in dataset.class_to_idx.items()}\n",
    "    \n",
    "    idx_distribution = dict(Counter(dataset.targets))\n",
    "    \n",
    "    for k,v in idx_distribution.items():\n",
    "        print(idx_to_class[k], \" : \",v)\n",
    "\n",
    "# transformation to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "                        transforms.Resize((224, 224)),\n",
    "                        transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'train', transform=transform)\n",
    "print(\"Train data...\\n\")\n",
    "data_distribution(train_dataset)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "valid_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'splittedVal', transform=transform)\n",
    "print(\"Valid data...\\n\")\n",
    "data_distribution(valid_dataset)\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'val', transform=transform)\n",
    "print(\"Test data...\\n\")\n",
    "data_distribution(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "valid_loader=DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "idx_to_class = {v:k for k, v in train_dataset.class_to_idx.items()} # mapping from id to class_name\n",
    "\n",
    "xx,yy=next(iter(train_loader))\n",
    "#plotting sample images\n",
    "fig=plt.figure(figsize=(25,10))\n",
    "for i in range(1,11):\n",
    "    ax=fig.add_subplot(2,5,i)\n",
    "    x=xx[i]\n",
    "    y=yy[i]\n",
    "    x=x.permute(1,2,0)\n",
    "    ax.imshow(x,cmap='gray')\n",
    "    ax.set_title(idx_to_class[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-09T09:59:52.352980Z",
     "iopub.status.busy": "2023-04-09T09:59:52.352504Z",
     "iopub.status.idle": "2023-04-09T10:01:02.174649Z",
     "shell.execute_reply": "2023-04-09T10:01:02.173217Z",
     "shell.execute_reply.started": "2023-04-09T09:59:52.352929Z"
    }
   },
   "outputs": [],
   "source": [
    "def getActivation(function): # activation functions\n",
    "    if function=='ReLU':\n",
    "        return nn.ReLU()\n",
    "    if function=='GELU':\n",
    "        return nn.GELU()\n",
    "    if function=='SiLU':\n",
    "        return nn.SELU()\n",
    "    if function=='Mish':\n",
    "        return nn.Mish()\n",
    "    return nn.ReLU() # if no match\n",
    "    \n",
    "    \n",
    "# Building Model\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super().__init__() \n",
    "        self.learning_rate=config.learning_rate\n",
    "        layers=[]\n",
    "        input_channels=3 \n",
    "        num_layers=5 # some values are hardcoded here as we are not inteneded to run sweeep on them. But overwriting of these values is supported through train.py\n",
    "        kernel_size=3\n",
    "        kernel_stride=1\n",
    "        max_pool_size=2\n",
    "        max_pool_stride=2\n",
    "        filters=[]\n",
    "        # based on filter org,determining number fo filters at each layer\n",
    "        if(config.filter_organization=='same'):\n",
    "            filters=[config.filter_size]*num_layers\n",
    "        elif(config.filter_organization=='double'):\n",
    "            filters.append(config.filter_size)\n",
    "            for i in range(4):\n",
    "                filters.append(filters[-1]*2)\n",
    "        elif(config.filter_organization=='halve'):\n",
    "            filters.append(config.filter_size)\n",
    "            for i in range(4):\n",
    "                filters.append(filters[-1]//2)\n",
    "\n",
    "        filters.insert(0,input_channels)\n",
    "        out_height=224 # input shape\n",
    "        for i in range(5):\n",
    "            layers.append(nn.Conv2d(filters[i],filters[i+1],kernel_size = kernel_size)) # conv layer\n",
    "            out_height=(out_height-kernel_size)//kernel_stride+1 # output shape of conv layer\n",
    "            layers.append(nn.MaxPool2d(kernel_size = max_pool_size,stride = max_pool_stride)) # max poolinglayer \n",
    "            out_height=out_height//max_pool_stride # output size of maxpool layer\n",
    "            layers.append(getActivation(config.activation))\n",
    "            if(config.batch_normalisation=='Yes'):# if batch norm is enabled then add accordingly.\n",
    "                layers.append(nn.BatchNorm2d(filters[i+1]))\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Dropout(config.dropout)) # dropout at FC layer\n",
    "        layers.append(nn.Linear(out_height*out_height*filters[-1],config.dense_layer_size))\n",
    "        layers.append(nn.Linear(config.dense_layer_size,10)) # out put layer\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.valid_loss=[]\n",
    "        self.valid_acc=[]\n",
    "        self.train_loss=[]\n",
    "        self.train_acc=[]\n",
    "        \n",
    "  \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x) # passes input x to sequentially through all the layers and output is obtained from last layer\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr= self.learning_rate) # setting up adam optimizer\n",
    "\n",
    "    def training_step(self,batch,batch_idx): # After every train batch, computes it's loss/acc and store it.\n",
    "        X,Y = batch\n",
    "        output = self(X)\n",
    "        loss = self.loss(output,Y)\n",
    "        acc = (output.argmax(dim = 1) == Y).float().mean()\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_acc.append(acc)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch,batch_idx): # After every valid batch, computes it's loss/acc and store it.\n",
    "        X,Y = batch\n",
    "        output = self(X)\n",
    "        loss = self.loss(output,Y)\n",
    "        acc = (output.argmax(dim = 1) == Y).float().mean()\n",
    "        self.valid_loss.append(loss)\n",
    "        self.valid_acc.append(acc)\n",
    "        return loss\n",
    "    def on_train_epoch_end(self): #once an epoch is completed, print and log the metrics to WandB\n",
    "        valid_loss=sum(self.valid_loss)/len(self.valid_loss)\n",
    "        valid_acc=sum(self.valid_acc)/len(self.valid_acc)\n",
    "        train_loss=sum(self.train_loss)/len(self.train_loss)\n",
    "        train_acc=sum(self.train_acc)/len(self.train_acc)\n",
    "        self.train_acc=[]\n",
    "        self.train_loss=[]\n",
    "        self.valid_loss=[]\n",
    "        self.valid_acc=[]\n",
    "        print(f\"Epoch: {self.current_epoch} train accuracy :{train_acc:.2f} valid_accuracy :{valid_acc:.2f}\")\n",
    "        \n",
    "\n",
    "        wandb.log({'train_acc':train_acc,'train_loss':train_loss,'valid_acc':valid_acc,'valid_loss':valid_loss})\n",
    "  \n",
    "  \n",
    "\n",
    "config= {\n",
    "    'method': 'bayes',\n",
    "    'name': 'CNN Assign 2',\n",
    "    'metric': {\n",
    "        'goal': 'maximize', \n",
    "        'name': 'valid_acc'\n",
    "      },\n",
    "    \"parameters\":\n",
    "    {\n",
    "    \"filter_size\":{\n",
    "      \"values\" :[32,64]\n",
    "    },\n",
    "    \"data_augumentation\" :{\n",
    "        \"values\" : [\"Yes\",\"No\"]\n",
    "    },\n",
    "    \"batch_normalisation\" :{\n",
    "        \"values\" : [\"Yes\",\"No\"]\n",
    "    },\n",
    "    \"dropout\" :{\n",
    "        \"values\" : [0,0.1]\n",
    "    },\n",
    "    \n",
    "     \"filter_organization\" :{\n",
    "        \"values\" : ['same','halve','double']\n",
    "    },\n",
    "    \n",
    "    \"activation\" :{\n",
    "          \"values\" : [\"ReLU\",\"GELU\",\"SiLU\",\"Mish\"]\n",
    "    },\n",
    "    \"epochs\" :{\n",
    "          \"values\" : [10]\n",
    "    },\n",
    "    \"dense_layer_size\" :{\n",
    "          \"values\" : [256,128]\n",
    "    },\n",
    "      \"learning_rate\" :{\n",
    "          \"values\" : [0.0001]\n",
    "    }\n",
    "    \n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with wandb.init(save_code=False) as run:\n",
    "        params=dict(wandb.config)\n",
    "        params=SimpleNamespace(**params)\n",
    "        run_name=f'FZ-{params.filter_size} AF - {params.activation} filter_org- {params.filter_organization} batch_norm -{params.batch_normalisation} data_aug -{params.data_augumentation} dropout- {params.dropout}'\n",
    "        wandb.run.name=run_name\n",
    "        if(params.data_augumentation=='Yes'): # loading train data based on data augumentation is enabled or not.\n",
    "            transform = transforms.Compose([\n",
    "                        transforms.Resize((224, 224)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.RandomRotation(10),\n",
    "                       ])\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                        transforms.Resize((224, 224)),\n",
    "                        transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "            \n",
    "        \n",
    "        train_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'train', transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        model = Model(params) \n",
    "        trainer = pl.Trainer(max_epochs=params.epochs,devices=1,accelerator=\"gpu\") \n",
    "        trainer.fit(model,train_loader,valid_loader) # fitting the model.\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=config, project='CNN_V2')\n",
    "\n",
    "wandb.agent(sweep_id, main, count=40)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T10:01:02.178409Z",
     "iopub.status.busy": "2023-04-09T10:01:02.177962Z",
     "iopub.status.idle": "2023-04-09T10:01:02.183930Z",
     "shell.execute_reply": "2023-04-09T10:01:02.182500Z",
     "shell.execute_reply.started": "2023-04-09T10:01:02.178358Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
