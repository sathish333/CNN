{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key='24434976526d9265fdbe2b2150787f46522f5da4')\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nimport shutil\nimport os\nimport random\nimport pytorch_lightning as pl\nfrom types import SimpleNamespace\n\nfrom pytorch_lightning.loggers import WandbLogger\n%matplotlib inline\n\nprefix='/kaggle/input/inaturalist12k/Data/inaturalist_12K/'\n\ndata_prefix='/kaggle/working/'\nclasses=['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n\n    \nflag=os.path.exists(data_prefix+'splittedVal')\n\n\n\nvalid_split=0.2\nif not flag:\n    for each in ['train','val']:\n        shutil.copytree(prefix+each,data_prefix+each)\n    os.mkdir(data_prefix+\"splittedVal\")\n    for each in classes:\n        images = os.listdir(data_prefix+'train/'+each+'/')\n        random.shuffle(images)\n        valid_till=int(len(images)*valid_split)\n        os.mkdir(data_prefix+'splittedVal/'+each)\n        for i in range(valid_till):\n            shutil.move(data_prefix+'train/'+each+\"/\"+images[i],data_prefix+'splittedVal/'+each)\n            \n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-09T09:57:47.060369Z","iopub.execute_input":"2023-04-09T09:57:47.060705Z","iopub.status.idle":"2023-04-09T09:59:49.932773Z","shell.execute_reply.started":"2023-04-09T09:57:47.060672Z","shell.execute_reply":"2023-04-09T09:59:49.931619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_distribution(dataset):\n    \n    print(\"Total number of samples :\",len(dataset.targets))\n    print(\"Number of classes :\",len(set(dataset.targets)),'\\n')\n    idx_to_class = {v:k for k, v in dataset.class_to_idx.items()}\n    \n    idx_distribution = dict(Counter(dataset.targets))\n    \n    for k,v in idx_distribution.items():\n        print(idx_to_class[k], \" : \",v)\n\ntransform = transforms.Compose([\n                        transforms.Resize((224, 224)),\n                        transforms.ToTensor(),\n                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n                        ])\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'train', transform=transform)\nprint(\"Train data...\\n\")\ndata_distribution(train_dataset)\n\ntrain_size = int(0.8 * len(train_dataset))\nvalid_size = len(train_dataset) - train_size\nvalid_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'splittedVal', transform=transform)\nprint(\"Valid data...\\n\")\ndata_distribution(valid_dataset)\n\ntest_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'val', transform=transform)\nprint(\"Test data...\\n\")\ndata_distribution(test_dataset)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nvalid_loader=DataLoader(valid_dataset, batch_size=16, shuffle=False)\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nidx_to_class = {v:k for k, v in train_dataset.class_to_idx.items()}\n\nxx,yy=next(iter(train_loader))\nfig=plt.figure(figsize=(25,10))\nfor i in range(1,11):\n    ax=fig.add_subplot(2,5,i)\n    x=xx[i]\n    y=yy[i]\n    x=x.permute(1,2,0)\n    ax.imshow(x,cmap='gray')\n    ax.set_title(idx_to_class[y.item()])","metadata":{"execution":{"iopub.status.busy":"2023-04-09T09:59:49.934893Z","iopub.execute_input":"2023-04-09T09:59:49.935665Z","iopub.status.idle":"2023-04-09T09:59:52.350798Z","shell.execute_reply.started":"2023-04-09T09:59:49.935612Z","shell.execute_reply":"2023-04-09T09:59:52.349351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getActivation(function):\n    if function=='ReLU':\n        return nn.ReLU()\n    if function=='GELU':\n        return nn.GELU()\n    if function=='SiLU':\n        return nn.SELU()\n    if function=='Mish':\n        return nn.Mish()\n    return nn.ReLU() # if no match\n    \n    \n\nclass Model(pl.LightningModule):\n    def __init__(self,config):\n        \n        super().__init__()\n        self.learning_rate=config.learning_rate\n        layers=[]\n        input_channels=3\n        num_layers=5\n        kernel_size=3\n        kernel_stride=1\n        max_pool_size=2\n        max_pool_stride=2\n        filters=[]\n        if(config.filter_organization=='same'):\n            filters=[config.filter_size]*num_layers\n        elif(config.filter_organization=='double'):\n            filters.append(config.filter_size)\n            for i in range(4):\n                filters.append(filters[-1]*2)\n        elif(config.filter_organization=='halve'):\n            filters.append(config.filter_size)\n            for i in range(4):\n                filters.append(filters[-1]//2)\n\n        filters.insert(0,input_channels)\n        out_height=224\n        for i in range(5):\n            layers.append(nn.Conv2d(filters[i],filters[i+1],kernel_size = kernel_size))\n            out_height=(out_height-kernel_size)//kernel_stride+1\n            layers.append(nn.MaxPool2d(kernel_size = max_pool_size,stride = max_pool_stride))  \n            out_height=out_height//max_pool_stride\n            layers.append(getActivation(config.activation))\n            if(config.batch_normalisation=='Yes'):\n                layers.append(nn.BatchNorm2d(filters[i+1]))\n        layers.append(nn.Flatten())\n        layers.append(nn.Dropout(config.dropout))\n        layers.append(nn.Linear(out_height*out_height*filters[-1],config.dense_layer_size))\n        layers.append(nn.Linear(config.dense_layer_size,10))\n        self.net = nn.Sequential(*layers)\n        self.loss = nn.CrossEntropyLoss()\n        self.valid_loss=[]\n        self.valid_acc=[]\n        self.train_loss=[]\n        self.train_acc=[]\n        \n  \n        \n    def forward(self,x):\n        return self.net(x)\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(),lr= self.learning_rate)\n\n    def training_step(self,batch,batch_idx):\n        X,Y = batch\n        output = self(X)\n        loss = self.loss(output,Y)\n        acc = (output.argmax(dim = 1) == Y).float().mean()\n        self.train_loss.append(loss)\n        self.train_acc.append(acc)\n        return loss\n\n\n    def validation_step(self, batch,batch_idx):\n        X,Y = batch\n        output = self(X)\n        loss = self.loss(output,Y)\n        acc = (output.argmax(dim = 1) == Y).float().mean()\n        self.valid_loss.append(loss)\n        self.valid_acc.append(acc)\n        return loss\n    def on_train_epoch_end(self):\n        valid_loss=sum(self.valid_loss)/len(self.valid_loss)\n        valid_acc=sum(self.valid_acc)/len(self.valid_acc)\n        train_loss=sum(self.train_loss)/len(self.train_loss)\n        train_acc=sum(self.train_acc)/len(self.train_acc)\n        self.train_acc=[]\n        self.train_loss=[]\n        self.valid_loss=[]\n        self.valid_acc=[]\n        print(f\"Epoch: {self.current_epoch} train accuracy :{train_acc:.2f} valid_accuracy :{valid_acc:.2f}\")\n        \n\n        wandb.log({'train_acc':train_acc,'train_loss':train_loss,'valid_acc':valid_acc,'valid_loss':valid_loss})\n  \n  \n\nconfig= {\n    'method': 'bayes',\n    'name': 'CNN Assign 2',\n    'metric': {\n        'goal': 'maximize', \n        'name': 'valid_acc'\n      },\n    \"parameters\":\n    {\n    \"filter_size\":{\n      \"values\" :[32,64]\n    },\n    \"data_augumentation\" :{\n        \"values\" : [\"Yes\",\"No\"]\n    },\n    \"batch_normalisation\" :{\n        \"values\" : [\"Yes\",\"No\"]\n    },\n    \"dropout\" :{\n        \"values\" : [0,0.1]\n    },\n    \n     \"filter_organization\" :{\n        \"values\" : ['same','halve','double']\n    },\n    \n    \"activation\" :{\n          \"values\" : [\"ReLU\",\"GELU\",\"SiLU\",\"Mish\"]\n    },\n    \"epochs\" :{\n          \"values\" : [10]\n    },\n    \"dense_layer_size\" :{\n          \"values\" : [256,128]\n    },\n      \"learning_rate\" :{\n          \"values\" : [0.0001]\n    }\n    \n  }\n\n}\n\n\ndef main():\n    with wandb.init(save_code=False) as run:\n        params=dict(wandb.config)\n        params=SimpleNamespace(**params)\n        run_name=f'FZ-{params.filter_size} AF - {params.activation} filter_org- {params.filter_organization} batch_norm -{params.batch_normalisation} data_aug -{params.data_augumentation} dropout- {params.dropout}'\n        wandb.run.name=run_name\n        if(params.data_augumentation=='Yes'):\n            transform = transforms.Compose([\n                        transforms.Resize((224, 224)),\n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.RandomRotation(10),\n                       ])\n        else:\n            transform = transforms.Compose([\n                        transforms.Resize((224, 224)),\n                        transforms.ToTensor(),\n                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n                        ])\n            \n        \n        train_dataset = torchvision.datasets.ImageFolder(root=data_prefix+'train', transform=transform)\n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n        model = Model(params) \n        trainer = pl.Trainer(max_epochs=params.epochs,devices=1,accelerator=\"gpu\") \n        trainer.fit(model,train_loader,valid_loader)\n\nsweep_id = wandb.sweep(sweep=config, project='CNN_V2')\n\nwandb.agent(sweep_id, main, count=40)\nwandb.finish()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-09T09:59:52.352504Z","iopub.execute_input":"2023-04-09T09:59:52.352980Z","iopub.status.idle":"2023-04-09T10:01:02.174649Z","shell.execute_reply.started":"2023-04-09T09:59:52.352929Z","shell.execute_reply":"2023-04-09T10:01:02.173217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-09T10:01:02.177962Z","iopub.execute_input":"2023-04-09T10:01:02.178409Z","iopub.status.idle":"2023-04-09T10:01:02.183930Z","shell.execute_reply.started":"2023-04-09T10:01:02.178358Z","shell.execute_reply":"2023-04-09T10:01:02.182500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}